{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LamoomAI CICD - Multiple Test Runs Example\n",
    "\n",
    "This notebook demonstrates how to use the enhanced LamoomAI CICD library to:\n",
    "1. Generate test cases from ideal answers\n",
    "2. Save test cases to JSON files\n",
    "3. Run multiple tests against the same test case\n",
    "4. Aggregate and visualize test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kateyanchenka/projects/lamoom-cicd/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import lamoom_cicd\n",
    "from lamoom_cicd import TestLLMResponsePipe, TestCase, AggregatedResult\n",
    "\n",
    "# Load environment variables for API keys\n",
    "load_dotenv()\n",
    "openai_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize the test pipeline\n",
    "lamoom_pipe = TestLLMResponsePipe(openai_key=openai_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Sample Data\n",
    "\n",
    "We'll use a simple example about blockchain to demonstrate the functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('./test_data/medical_questions_answers.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a Test Case\n",
    "\n",
    "There are two ways to create test cases: either by generating tests directly from the ideal answer, or by running a comparison first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Method 1: Generate Tests Directly\n",
    "\n",
    "Use the `generate_tests` method to create test questions directly from an ideal answer without running a comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test questions directly from ideal answer\n",
    "generated = lamoom_pipe.generate_tests(\n",
    "    ideal_answer=ideal_answer,\n",
    "    optional_params={\n",
    "        \"prompt_id\": \"blockchain_direct\",\n",
    "        \"context\": {\"user_query\": \"Explain blockchain technology to a beginner\"},\n",
    "        \"prompt_data\": \"Explain blockchain in simple terms, {user}.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(generated['test_questions'])} test questions for prompt ID: {generated['prompt_id']}\")\n",
    "print(\"\\nExample questions:\")\n",
    "for i, q in enumerate(generated['test_questions'][:3]):  # Show the first 3 questions\n",
    "    print(f\"\\nQuestion {i+1}: {q['test_question']}\")\n",
    "    print(f\"Ideal Answer: {q['ideal_answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a test case directly from the generated questions\n",
    "test_case = lamoom_pipe.create_test_case(\n",
    "    ideal_answer=ideal_answer,\n",
    "    optional_params={\n",
    "        \"prompt_id\": \"blockchain_direct\",\n",
    "        \"context\": {\"user_query\": \"Explain blockchain technology to a beginner\"}\n",
    "    }\n",
    ")\n",
    "\n",
    "# Save to a JSON file\n",
    "test_cases_file = \"blockchain_test_cases.json\"\n",
    "lamoom_pipe.save_test_cases_to_json(test_cases_file)\n",
    "\n",
    "case_id = test_case.case_id\n",
    "print(f\"Test case created directly with ID: {case_id}\")\n",
    "print(f\"Test case saved to {test_cases_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Method 2: Create Test Case After Comparison\n",
    "\n",
    "Alternatively, you can run a comparison first and then save the results as a test case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run initial test to generate questions and compare\n",
    "result = lamoom_pipe.compare(\n",
    "    ideal_answer=ideal_answer,\n",
    "    llm_response=llm_responses[0],  # Use the first response for initial test\n",
    "    optional_params={\"prompt_id\": \"blockchain_example\"}\n",
    ")\n",
    "\n",
    "# Display the test results\n",
    "print(f\"Test Score: {result.score.score}% (Passed: {result.score.passed})\")\n",
    "print(\"\\nTest Questions:\")\n",
    "for q in result.questions:\n",
    "    print(f\"\\nQ: {q.test_question}\")\n",
    "    print(f\"Ideal Answer: {q.ideal_answer}\")\n",
    "    print(f\"LLM Answer: {q.llm_answer}\")\n",
    "    print(f\"Matches: {q.does_match_ideal_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the test case from comparison results\n",
    "comparison_test_case = lamoom_pipe.save_test_case(result)\n",
    "comparison_case_id = comparison_test_case.case_id\n",
    "\n",
    "# Update the JSON file with both test cases\n",
    "lamoom_pipe.save_test_cases_to_json(test_cases_file)\n",
    "\n",
    "print(f\"Comparison test case saved with ID: {comparison_case_id}\")\n",
    "print(f\"Updated test cases in {test_cases_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Multiple Tests with the Same Test Case\n",
    "\n",
    "Now we'll run tests with all our different LLM responses against the same test case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run multiple tests with different responses\n",
    "results = lamoom_pipe.run_multiple_tests(case_id, llm_responses)\n",
    "\n",
    "# Display results\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Run {i+1}: Score {result.score.score}% (Passed: {result.score.passed})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Test Cases from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new test pipeline and load the test cases\n",
    "new_pipe = TestLLMResponsePipe(openai_key=openai_key)\n",
    "loaded_cases = new_pipe.load_test_cases_from_json(test_cases_file)\n",
    "\n",
    "print(f\"Loaded {len(loaded_cases)} test cases:\")\n",
    "for case in loaded_cases:\n",
    "    print(f\"\\nCase ID: {case.case_id}\")\n",
    "    print(f\"Prompt ID: {case.prompt_id}\")\n",
    "    print(f\"Number of questions: {len(case.test_questions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Test Cases from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample CSV file\n",
    "csv_file = \"sample_test_cases.csv\"\n",
    "\n",
    "# Write the CSV content\n",
    "with open(csv_file, 'w') as f:\n",
    "    f.write(\"ideal_answer,llm_response,optional_params\\n\")\n",
    "    f.write(f'\"{ideal_answer}\",\"\",\"{{\\\"prompt_id\\\": \\\"blockchain_from_csv\\\"}}\"\\n')\n",
    "    f.write('\"Artificial Intelligence is software that can learn and adapt.\",\"\",\"{{\\\"prompt_id\\\": \\\"ai_definition\\\"}}\"\\n')\n",
    "\n",
    "# Generate test cases from the CSV\n",
    "output_json = \"generated_test_cases.json\"\n",
    "cases = new_pipe.save_test_cases_from_csv(csv_file, output_json)\n",
    "\n",
    "print(f\"Generated {len(cases)} test cases and saved to {output_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save and Load Test Results"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Unified Test Runner\n\nThe `run_tests` method provides a streamlined way to run tests with various configurations.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Save LLM responses for later use\nresponses_file = \"llm_responses.json\"\nresponses_dict = {\n    case_id: llm_responses  # Use the same responses for our test case\n}\nlamoom_pipe.save_llm_responses(responses_dict, responses_file)\nprint(f\"Saved LLM responses to {responses_file}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Method 1: Run tests from a test cases file with LLM responses\nprint(\"Method 1: Running tests from test cases file...\")\nresults1 = new_pipe.run_tests(\n    llm_response_provider=responses_file,  # Load responses from file\n    test_cases=test_cases_file,            # Load test cases from file\n    results_file=\"run_method1_results.json\"\n)\n\nprint(f\"\\nSummary:\")\nprint(f\"Total tests: {results1['summary']['total_tests']}\")\nprint(f\"Average score: {results1['summary']['avg_score']:.2f}%\")\nprint(f\"Pass rate: {results1['summary']['overall_pass_rate']:.2f}%\")\nprint(f\"Time taken: {results1['summary']['elapsed_time']:.2f} seconds\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Method 2: Run tests with a list of LLM responses\nprint(\"\\nMethod 2: Running tests with a list of responses...\")\nresults2 = new_pipe.run_tests(\n    llm_response_provider=llm_responses[:2],  # Use just 2 responses\n    test_cases=loaded_cases,                  # Use loaded test cases\n    runs_per_case=2,                          # Run each case twice\n    results_file=\"run_method2_results.json\"\n)\n\nprint(f\"\\nSummary:\")\nprint(f\"Total tests: {results2['summary']['total_tests']}\")\nprint(f\"Average score: {results2['summary']['avg_score']:.2f}%\")\nprint(f\"Pass rate: {results2['summary']['overall_pass_rate']:.2f}%\")\nprint(f\"Time taken: {results2['summary']['elapsed_time']:.2f} seconds\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Method 3: Run tests with a dictionary mapping case_ids to specific responses\nprint(\"\\nMethod 3: Running tests with a dictionary of responses...\")\n\n# Create a dictionary of case_id to specific responses\ncustom_responses = {\n    case_id: [llm_responses[0], llm_responses[1]],  # First test case gets first two responses\n    comparison_case_id: [llm_responses[2], llm_responses[3]]  # Second test case gets next two\n}\n\nresults3 = new_pipe.run_tests(\n    llm_response_provider=custom_responses,  # Dictionary mapping case_ids to responses\n    results_file=\"run_method3_results.json\"\n)\n\nprint(f\"\\nSummary:\")\nprint(f\"Total tests: {results3['summary']['total_tests']}\")\nprint(f\"Average score: {results3['summary']['avg_score']:.2f}%\")\nprint(f\"Pass rate: {results3['summary']['overall_pass_rate']:.2f}%\")\nprint(f\"Time taken: {results3['summary']['elapsed_time']:.2f} seconds\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Visualize all the results\nplt.figure(figsize=(14, 8))\nplt.subplots_adjust(bottom=0.3)  # Make room for the case_id labels\n\nall_aggregated = results3[\"aggregated\"]  # Use the most recent results with all cases\ncase_ids = [agg.case_id for agg in all_aggregated]\navg_scores = [agg.avg_score for agg in all_aggregated]\nstd_devs = [agg.std_deviation for agg in all_aggregated]\npass_rates = [agg.pass_rate for agg in all_aggregated]\n\n# Plot average scores with error bars\nplt.bar(\n    range(len(case_ids)),\n    avg_scores,\n    yerr=std_devs,\n    capsize=5,\n    alpha=0.7,\n    color='skyblue'\n)\n\n# Add pass rates as labels\nfor i, (avg, pr) in enumerate(zip(avg_scores, pass_rates)):\n    plt.text(i, avg + std_devs[i] + 2, f\"{pr:.0f}% pass\", \n             ha='center', va='bottom', fontweight='bold')\n\n# Add threshold line\nplt.axhline(y=new_pipe.threshold, color='r', linestyle='--', \n            label=f\"Passing threshold ({new_pipe.threshold}%)\")\n\nplt.title(\"Test Results Across All Cases\")\nplt.xlabel(\"Test Case\")\nplt.ylabel(\"Average Score (%)\")\nplt.xticks(range(len(case_ids)), case_ids, rotation=45)\nplt.ylim(0, 100)\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Save test results to JSON\nresults_file = \"test_results.json\"\nlamoom_pipe.save_test_results_to_json(results_file)\n\n# Load results in a new pipe\nanother_pipe = TestLLMResponsePipe()\nloaded_results = another_pipe.load_test_results_from_json(results_file)\n\nprint(f\"Loaded {len(loaded_results)} test results\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}